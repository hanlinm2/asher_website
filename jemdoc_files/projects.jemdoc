# jemdoc: menu{MENU}{projects.html}
= Projects
This page contains all my non-publication projects, including course 
projects, hackathons, personal projects and undergrad research.

== 2023

~~~
{}{img_left}{./images/hog.jpg}{}{280px}{}{}
== Classical Method on New Datasets: Pedestrian Detection with HOG and CNN
=== Asher Mai, Matthew Tang, Sid Ahuja
==== CS 598 LAZ: Computer Vision: What Will Stand the Test of Time?
Neural network models are becoming increasingly more powerful, but also taking longer
and more computing resources to train. We aim to reduce the training time by using Histogram
of Oriented Gradients (HOG) to preprocess images before training. Our experiments with
pedestrian detection task show a 125x speed up in training time without sacrificing much accuracy. 

[./assets/HoG_Presentation.pdf Presentation] \/
[./assets/HOG_Project.pdf Final Report] \/
[https://github.com/mhtang2/HOGPedestrianDetection/ Code]
~~~

~~~
{}{video_left}{./images/HRI.mp4}{}{280px}{}{}
== Hand Gesture Robot Control 
=== Asher Mai, Joshua Cox, Ian Xu
==== ECE 598: Human Robot Interaction
We use a fully vision based approach to enable gesture based robot control. We
demonstrate drawing and pick-and-place tasks using this method. While the right hand
controls how the robot's end-effector moves (x and y direction), 
the left hand controls up and down motion (z direction),
as well as turning on and off the vacuum gripper depending on the various gestures.

[./assets/598finalpresentation.pdf Presentation] \/
[./assets/hri_paper.pdf Final report]
~~~

~~~
{}{video_left}{./images/crowd_nav.mp4}{}{280px}{}{}{}
== Crowd Navigation with Occlusion-Aware Inference Using People as Sensors
=== Asher Mai, [https://yejimun.github.io Ye-Ji Mun], Prof Katie Driggs-Campbell
==== Undergrad Research
By training a variational auto-encoder (VAE) to generate occupancy grid maps (OGM) that
try to estimate occluded pedestrians' locations given sensor OGM data that don't have these occluded pedestrians, 
we effectively use people's interaction for occlusion inference in crowd 
navigation tasks. My contribution includes,
. Using blob detection to evaluate accuracy of generated OGM compared to ground truth OGM
and assessing the network's ability to see pedestrians that the sensor doesn't see. 
. Integrating baseline Astar planner with the occlusion inference module on the 
[https://paperswithcode.com/dataset/ucy UCY dataset].

This project is an extension to this [https://arxiv.org/abs/2210.00552 paper].
~~~

~~~
{}{video_left}{./images/rivian.mp4}{}{280px}{}{}{rivian}
== Rivian Adventure Retro Game
=== Asher Mai, Advaith Bala, David Chen
==== [https://github.com/uirphack/2023 2023 UIUC Research Park Hackathon]\n
==== /1st place and Best Company Incorporation Award/
Rivian Adventure is a retro game that incorporates many aspects of the company:
- Go offroad to activate camping mode and take advantage of Rivian's adventure focused trucks
- Go turbo mode and experience the fast acceleration of the R1T
- Drive through charging stations and don't worry about running out of battery
- When the truck breaks down, use Rivian Service Centers and you'll be back on the road in no time

[https://github.com/advaithbala/uirphack_2023 Code] \/ 
[https://youtu.be/4WL72N0womw Youtube] \/ 
[https://researchpark.illinois.edu/article/first-research-park-hackathon-paid-homage-to-retro-games/ Article]
~~~


~~~
{}{video_left}{./images/head_controlled_mouse.mp4}{}{280px}{}{}
== Head-Controlled Mouse
=== Amanda Favila, Asher Mai, Lauren Wilcox
==== ECE 445: Senior Design
We developed a device that allows a user with inability to move parts of their body to control
a mouse cursor using their head. We use an accelerometer and gyroscope to capture the movement
of the head, and translate it into mouse movement data that is sent by Bluetooth to the user's
desktop / laptop device. To left click or right click, simply hold the cursor in the place you
want to click, wait for the mouse to change into a crosshair shape, then tilt your head left or
right to perform left or right clicks.

[https://gitlab.com/ece445-team44/ece445_project Code] \/ 
[https://www.youtube.com/watch?v=HxvXf0QYEtk Youtube] \/ 
[./assets/ECE445_Final_Report.pdf Final Report] \/ 
[./assets/Team44_FinalPPT.pdf Presentation]
~~~


== 2022
~~~
{}{img_left}{./images/digital_notes.jpg}{}{280px}{}{}
== Digital Notes With Any Pen on Any Surface
=== Asher Mai, Pauline Lu
==== CS 445: Computational Photography
==== /[https://ece.illinois.edu/academics/ugrad/scholarships-and-awards/awards/bit Donald L. Bitzer and H. Gene Slottow Creativity Award]/
What if you want to draw or take notes digitally, but you don't have the money to buy an expensive set of tablet and stylus?
We designed a system that allows you to use any pen to draw on any surface using just your laptop's webcam.
First we do calibration to account for the angle of the webcam with relations to the surface in front of it using a piece 
of paper with green tapes on the four corners. We then use color thresholding to recognize the
green tip of the pen and its location on the surface, and translate it to location on screen using homography transform.

[https://gitlab.engr.illinois.edu/hanlinm2/digital-notes Code] \/ 
[https://www.youtube.com/watch?v=yeBxkBcJyF4 Youtube] \/ 
[./assets/CS445_Final_Project_Report.pdf Final Report]
~~~


~~~
{}{img_left}{./images/gesture.png}{}{280px}{}{}
== Hand Tracking and Gesture Recognition for Automated VFX
=== Michelle Zhang, Asher Mai, Noah Franceschini, Aditi Tiwari
==== CS 543/ECE 549: Computer Vision
We compared two different methods for the gesture detection problem: classical method with convex hull, and learning based
method with MobileNetV3. We applied different visual effects based on the gestures detected:
- One finger: A flame will show up on your finger and tilt based on hand movement determined by optical flow
- Two fingers: A flame will show up between the two fingers. Up and down motion will change the color of the flame
- Three fingers: Toggle on/off a cartoon filter.

[https://github.com/mz32-personal/ECE549-Final-Project Code] \/ 
[https://www.youtube.com/watch?v=c_B_kCH9-h4 Youtube] \/ 
[./assets/ECE549_Final_Report.pdf Final Report]
~~~

~~~
{}{img_left}{./images/quantization.jpeg}{}{280px}{}{}{}
== Convolutional Neural Network Pruning and Quantization for FPGA
=== Chuxuan Hu, Hanlin “Asher” Mai, Jiaru Zou, Joseph Rejive, William Eustis, Prof. Volodymyr Kindratenko
==== Undergrad Research
We trained VGG16 image classification CNN using PyTorch and CIFAR10 dataset.
We were able to reduce the model size by more than 4x using pruning and PyTorch’s Post Training Quantization framework, 
and collaborated with FPGA team to integrate quantized 8 bit convolutional kernels for image classification on hardware.

[./assets/quantization.pdf Poster]
~~~

~~~
{}{video_left}{./images/yoyo.mp4}{}{280px}{}{}{yoyo}
== Automatic Visual Effect on Yoyo Trick Videos 
=== Asher Mai
==== Personal Project
I taught myself how to do cool yoyo tricks. I taught myself how to program in Python. And I combined these
two skills to add automatic visual effects to my yoyo videos. I used OpenCV CSRT object tracker to track
the position of the yoyo in each frame, and then add a trace of the path that the yoyo takes.

[https://github.com/hanlinm2/yoyo_tracking_vfx Code] \/
[https://www.linkedin.com/posts/ashermai_combing-my-python-and-yoyo-skills-i-was-activity-6930984293078888448-nuUo?utm_source=share&utm_medium=member_desktop More Video]
~~~

~~~
{}{video_left}{./images/Gem.mp4}{}{280px}{}{}{}
== GEM Vehicle Parking Navigation with Pipeline Design
=== Yan Miao, Arjun Ray, John Pohovey, Asher Mai
==== [https://publish.illinois.edu/safe-autonomy/ ECE 484: Principles of Safe Autonomy]
We designed a pipeline that enables autonomous parking lot navigation on the Gem Vehicle platform.
The pipeline includes perception with LiDAR and YOLOv5, path planning with Hybrid A-star, and control using PID.
We tested our system with different obstacles in the parking lot, and evaluated the parking success using 
Intersection Over Union (IOU) metric. We tested how well the PID control follows the planned trajectory using
Dynamic Time Warping (DTW).

[https://www.youtube.com/watch?list=PLcA4s4DKSOF12iLaJmmJ_2ZoCD6aFd9Ct&v=-f7Qd7TnaDU Youtube] \/
[https://docs.google.com/presentation/d/1qyt7nnVl4wSo1H0AG19ofFyk_EndlIzEOxcE05svHTk/edit?usp=sharing Presentation]
~~~

== 2021

~~~
{}{video_left}{./images/crossy.mp4}{}{280px}{}{}{}
== Crossy Road Game on FPGA
=== Asher Mai, Amanda Favila
==== ECE 385: Digital Systems Laboratory
We recreated the popular iOS game Crossy Road on FPGA using SystemVerilog HDL (Hardware Description Language).
We designed our own game graphics down to pixel level and output VGA signal to display a 60 Hz, 480 by 640 pixel game.
One of the challenges was to randomly generate and efficiently store the essential game information such as all the car and tree positions. 
After brainstorming, we came up with the idea of using linear-feedback shift registers to solve this challenge.

[https://www.linkedin.com/posts/ashermai_after-a-month-of-workamanda-favilaand-activity-6797002446129524736-JILB?utm_source=share&utm_medium=member_desktop LinkedIn Post]
~~~

~~~
"Thanks for writing to me and showing me this. It's nice to hear that you enjoy Crossy!
Your project looks awesome, especially on a big screen!
All the best for your course and future projects." --- [https://andrewsum.com Andrew Sum], Creator of Crossy Road
~~~

== 2020

~~~
{}{video_left}{./images/ball.mp4}{}{280px}{}{}{}
== Floating Glass Sphere
=== Asher Mai
==== Personal Project
I was experimenting with 3D animation, and I had this idea of combining
frosted glass and changing shape keys that animates the shape of both 
the inner and outer sphere.

Modeled and animated with [https://www.blender.org Blender].
~~~

~~~
{}{img_left}{./images/computer.JPG}{}{280px}{}{}{}
== The Future of Computing
=== Asher Mai
==== Personal Project
I had an imagination for what computers might look like in the future. You can have
windows floating around you, and you can drag them around. You can have multiple screens but 
without any monitors. This imagination has been brought to reality with the recent release
of Apple Vision Pro.

Modeled with [https://www.blender.org Blender].
~~~

== 2019

~~~
{}{img_left}{./images/endbound.jpeg}{}{280px}{}{}{}
== EndBound: A Calculus-based Board Game
=== Asher Mai, Ben Snyder, Jake Mulé, Jeremy Cheng
==== AP Calculus BC
We designed a board game similar to Chutes and Ladders, but instead of using
dice roll to determine how far you move, you use Function cards and EndBound cards
to calculate the integral that determines how far you move.

Steps to move =
\(
    \int\limits_{0}^{(EndBound\;Card)} (Function\;Card) \;dx
\)

[./assets/endBound_rules.pdf Rule Book]
~~~

~~~
{}{raw}
<div class="bg-image" style="display:block;"></div>
<style>
    .bg-image{
        background-attachment: fixed;
        background-size: 100%;
        width: 100%;
        height: 100%;
        display: none;
        position: fixed;
        top: 0;
        left: 0;
        background: #fff url("./images/deepmind3.jpg") center no-repeat;
        background-size: cover;
        background-position: center center;
        z-index: 0;
    }

</style>
<style>
    div#layout-content-container {
        background-color: #F5F5F7 !important ;
    }

    div#text-img-container {
        background-color: #FFFFFF;
        border-radius: 20px;
        padding:20px;
        box-shadow: 0px 0px 15px gray;
        box-sizing: border-box;
        border-radius: 20px;
        box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .6); 
        position: relative;
        z-index: 1;
        background: inherit;
        overflow: hidden;
    }
    div#text-img-container:before {
        content: "";
        position: absolute;
        /* background: inherit; */
        z-index: -1;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        box-shadow: inset 0 0 2000px rgba(255, 255, 255, .5);
        backdrop-filter: blur(30px) !important;
        -webkit-backdrop-filter: blur(20px) !important;
        margin: -20px;
    }

    div#layout-content :not(.bg-image) {
        z-index: 1;
        position: relative;
    }

    div#subtitle, div#footer-text, div#layout-content > h2, h1{
        background-color: #FFFFFF;
        border-radius: 20px;
        padding:10px;
        box-shadow: 0px 0px 15px gray;
        box-sizing: border-box;
        border-radius: 20px;
        box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .6); 
        position: relative;
        z-index: 1;
        background: inherit;
        overflow: hidden;
    }

    div#subtitle:before, div#footer-text:before, div#layout-content > h2:before, h1:before {
        content: "";
        position: absolute;
        background: inherit;
        z-index: -1;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        box-shadow: inset 0 0 2000px rgba(255, 255, 255, .5);
        backdrop-filter: blur(10px) !important;
        -webkit-backdrop-filter: blur(10px) !important;
        margin: -20px;
    }

    div#layout-content > h2, h1 {
        display:inline-block;
    }

</style>
<script>
    document.title = "Asher's Projects"
</script>
~~~

