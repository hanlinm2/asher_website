<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Asher Mai's Personal Website. Here you will see my education
background, projects, publications, updates, and learn more about me as a person.">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="./images/icon.png">
<link href="https://fonts.googleapis.com/css?family=Lato:100,100i,300,300i,400,400i,700,700i" rel="stylesheet">
<link rel="stylesheet" href="main.css" type="text/css" />
<!--- <title>Projects</title> --->
<title>Hanlin "Asher" Mai's Homepage</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
</head>
<body>
<div id="main-container">
<div id="header-container">
<div id="header">
<div id="header-icon-text-container">
<div id="header-icon-container" >
<a href="index.html"><img src="./images/icon.png" alt="" style="width: 100%; height: 100%; position: center; padding:0px; margin: 0px;"></a>
</div>
<div id="header-text-container">
<a href="profile.html">Hanlin "Asher" Mai</a>
</div>
</div>
<div class="navbar-item"><a href="publications.html">Publications</a></div>
<div class="navbar-item"><a href="projects.html">Projects</a></div>
<div class="navbar-item"><a href="assets/Asher_Resume.pdf" target="blank">Resume</a></div>
<div class="navbar-item"><a href="awards.html">Awards</a></div>
<div class="navbar-item"><a href="contact.html">Contact</a></div>
<div id="main">
<button id = "openButton" class="openbtn" onclick="openNav()">☰</button>
</div>
</div>
</div>
<div id="layout">
<div id="layout-menu-container">
<div id="layout-menu">
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="projects.html" class="current">Projects</a></div>
<div class="menu-item"><a href="assets/Asher_Resume.pdf" target="blank">Resume</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
<div class="menu-item"><a href="contact.html">Contact</a></div>
</div> <!-- <div id="layout-menu"> -->
</div> <!-- <div id="layout-menu-container"> -->
<div id="layout-content-container">
<div id="layout-content">
<div id="toptitle">
<h1>Projects</h1>
<div id="subtitle">This page contains all my non-publication projects, including course 
projects, hackathons, personal projects and undergrad research.</div>
</div>
<h2>2023</h2>
<div id="text-img-container"><div id="img-container">
<img src="./images/hog.jpg" alt="" width="280px" /></div>
<div id="text-container"><h2>Classical Method on New Datasets: Pedestrian Detection with HOG and CNN</h2>
<h3>Asher Mai, Matthew Tang, Sid Ahuja</h3>
<h4>CS 598 LAZ: Computer Vision: What Will Stand the Test of Time?</h4>
<p>Neural network models are becoming increasingly more powerful, but also taking longer
and more computing resources to train. We aim to reduce the training time by using Histogram
of Oriented Gradients (HOG) to preprocess images before training. Our experiments with
pedestrian detection task show a 125x speed up in training time without sacrificing much accuracy. </p>
<p><a href="./assets/HoG_Presentation.pdf" target=&ldquo;blank&rdquo;>Presentation</a> /
<a href="./assets/HOG_Project.pdf" target=&ldquo;blank&rdquo;>Final Report</a> /
<a href="https://github.com/mhtang2/HOGPedestrianDetection/" target=&ldquo;blank&rdquo;>Code</a></p>
</div></div>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px"><source src="./images/HRI.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>Hand Gesture Robot Control </h2>
<h3>Asher Mai, Joshua Cox, Ian Xu</h3>
<h4>ECE 598: Human Robot Interaction</h4>
<p>We use a fully vision based approach to enable gesture based robot control. We
demonstrate drawing and pick-and-place tasks using this method. While the right hand
controls how the robot's end-effector moves (x and y direction), 
the left hand controls up and down motion (z direction),
as well as turning on and off the vacuum gripper depending on the various gestures.</p>
<p><a href="./assets/598finalpresentation.pdf" target=&ldquo;blank&rdquo;>Presentation</a> /
<a href="./assets/hri_paper.pdf" target=&ldquo;blank&rdquo;>Final report</a></p>
</div></div>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px"><source src="./images/crowd_nav.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>Crowd Navigation with Occlusion-Aware Inference Using People as Sensors</h2>
<h3>Asher Mai, <a href="https://yejimun.github.io" target=&ldquo;blank&rdquo;>Ye-Ji Mun</a>, Prof Katie Driggs-Campbell</h3>
<h4>Undergrad Research</h4>
<p>By training a variational auto-encoder (VAE) to generate occupancy grid maps (OGM) that
try to estimate occluded pedestrians&rsquo; locations given sensor OGM data that don't have these occluded pedestrians, 
we effectively use people's interaction for occlusion inference in crowd 
navigation tasks. My contribution includes,</p>
<ol>
<li><p>Using blob detection to evaluate accuracy of generated OGM compared to ground truth OGM
and assessing the network's ability to see pedestrians that the sensor doesn't see. </p>
</li>
<li><p>Integrating baseline Astar planner with the occlusion inference module on the 
<a href="https://paperswithcode.com/dataset/ucy" target=&ldquo;blank&rdquo;>UCY dataset</a>.</p>
</li>
</ol>
<p>This project is an extension to this <a href="https://arxiv.org/abs/2210.00552" target=&ldquo;blank&rdquo;>paper</a>.</p>
</div></div>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px" id="rivian"><source src="./images/rivian.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>Rivian Adventure Retro Game</h2>
<h3>Asher Mai, Advaith Bala, David Chen</h3>
<h4><a href="https://github.com/uirphack/2023" target=&ldquo;blank&rdquo;>2023 UIUC Research Park Hackathon</a><br /></h4>
<h4><i>1st place and Best Company Incorporation Award</i></h4>
<p>Rivian Adventure is a retro game that incorporates many aspects of the company:</p>
<ul>
<li><p>Go offroad to activate camping mode and take advantage of Rivian's adventure focused trucks</p>
</li>
<li><p>Go turbo mode and experience the fast acceleration of the R1T</p>
</li>
<li><p>Drive through charging stations and don't worry about running out of battery</p>
</li>
<li><p>When the truck breaks down, use Rivian Service Centers and you'll be back on the road in no time</p>
</li>
</ul>
<p><a href="https://github.com/advaithbala/uirphack_2023" target=&ldquo;blank&rdquo;>Code</a> / 
<a href="https://youtu.be/4WL72N0womw" target=&ldquo;blank&rdquo;>Youtube</a> / 
<a href="https://researchpark.illinois.edu/article/first-research-park-hackathon-paid-homage-to-retro-games/" target=&ldquo;blank&rdquo;>Article</a></p>
</div></div>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px"><source src="./images/head_controlled_mouse.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>Head-Controlled Mouse</h2>
<h3>Amanda Favila, Asher Mai, Lauren Wilcox</h3>
<h4>ECE 445: Senior Design</h4>
<p>We developed a device that allows a user with inability to move parts of their body to control
a mouse cursor using their head. We use an accelerometer and gyroscope to capture the movement
of the head, and translate it into mouse movement data that is sent by Bluetooth to the user's
desktop / laptop device. To left click or right click, simply hold the cursor in the place you
want to click, wait for the mouse to change into a crosshair shape, then tilt your head left or
right to perform left or right clicks.</p>
<p><a href="https://gitlab.com/ece445-team44/ece445_project" target=&ldquo;blank&rdquo;>Code</a> / 
<a href="https://www.youtube.com/watch?v=HxvXf0QYEtk" target=&ldquo;blank&rdquo;>Youtube</a> / 
<a href="./assets/ECE445_Final_Report.pdf" target=&ldquo;blank&rdquo;>Final Report</a> / 
<a href="./assets/Team44_FinalPPT.pdf" target=&ldquo;blank&rdquo;>Presentation</a></p>
</div></div>
<h2>2022</h2>
<div id="text-img-container"><div id="img-container">
<img src="./images/digital_notes.jpg" alt="" width="280px" /></div>
<div id="text-container"><h2>Digital Notes With Any Pen on Any Surface</h2>
<h3>Asher Mai, Pauline Lu</h3>
<h4>CS 445: Computational Photography</h4>
<h4><i><a href="https://ece.illinois.edu/academics/ugrad/scholarships-and-awards/awards/bit" target=&ldquo;blank&rdquo;>Donald L. Bitzer and H. Gene Slottow Creativity Award</a></i></h4>
<p>What if you want to draw or take notes digitally, but you don't have the money to buy an expensive set of tablet and stylus?
We designed a system that allows you to use any pen to draw on any surface using just your laptop's webcam.
First we do calibration to account for the angle of the webcam with relations to the surface in front of it using a piece 
of paper with green tapes on the four corners. We then use color thresholding to recognize the
green tip of the pen and its location on the surface, and translate it to location on screen using homography transform.</p>
<p><a href="https://gitlab.engr.illinois.edu/hanlinm2/digital-notes" target=&ldquo;blank&rdquo;>Code</a> / 
<a href="https://www.youtube.com/watch?v=yeBxkBcJyF4" target=&ldquo;blank&rdquo;>Youtube</a> / 
<a href="./assets/CS445_Final_Project_Report.pdf" target=&ldquo;blank&rdquo;>Final Report</a></p>
</div></div>
<div id="text-img-container"><div id="img-container">
<img src="./images/gesture.png" alt="" width="280px" /></div>
<div id="text-container"><h2>Hand Tracking and Gesture Recognition for Automated VFX</h2>
<h3>Michelle Zhang, Asher Mai, Noah Franceschini, Aditi Tiwari</h3>
<h4>CS 543/ECE 549: Computer Vision</h4>
<p>We compared two different methods for the gesture detection problem: classical method with convex hull, and learning based
method with MobileNetV3. We applied different visual effects based on the gestures detected:</p>
<ul>
<li><p>One finger: A flame will show up on your finger and tilt based on hand movement determined by optical flow</p>
</li>
<li><p>Two fingers: A flame will show up between the two fingers. Up and down motion will change the color of the flame</p>
</li>
<li><p>Three fingers: Toggle on/off a cartoon filter.</p>
</li>
</ul>
<p><a href="https://github.com/mz32-personal/ECE549-Final-Project" target=&ldquo;blank&rdquo;>Code</a> / 
<a href="https://www.youtube.com/watch?v=c_B_kCH9-h4" target=&ldquo;blank&rdquo;>Youtube</a> / 
<a href="./assets/ECE549_Final_Report.pdf" target=&ldquo;blank&rdquo;>Final Report</a></p>
</div></div>
<div id="text-img-container"><div id="img-container">
<img src="./images/quantization.jpeg" alt="" width="280px" /></div>
<div id="text-container"><h2>Convolutional Neural Network Pruning and Quantization for FPGA</h2>
<h3>Chuxuan Hu, Hanlin “Asher” Mai, Jiaru Zou, Joseph Rejive, William Eustis, Prof. Volodymyr Kindratenko</h3>
<h4>Undergrad Research</h4>
<p>We trained VGG16 image classification CNN using PyTorch and CIFAR10 dataset.
We were able to reduce the model size by more than 4x using pruning and PyTorch’s Post Training Quantization framework, 
and collaborated with FPGA team to integrate quantized 8 bit convolutional kernels for image classification on hardware.</p>
<p><a href="./assets/quantization.pdf" target=&ldquo;blank&rdquo;>Poster</a></p>
</div></div>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px" id="yoyo"><source src="./images/yoyo.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>Automatic Visual Effect on Yoyo Trick Videos </h2>
<h3>Asher Mai</h3>
<h4>Personal Project</h4>
<p>I taught myself how to do cool yoyo tricks. I taught myself how to program in Python. And I combined these
two skills to add automatic visual effects to my yoyo videos. I used OpenCV CSRT object tracker to track
the position of the yoyo in each frame, and then add a trace of the path that the yoyo takes.</p>
<p><a href="https://github.com/hanlinm2/yoyo_tracking_vfx" target=&ldquo;blank&rdquo;>Code</a> /
<a href="https://www.linkedin.com/posts/ashermai_combing-my-python-and-yoyo-skills-i-was-activity-6930984293078888448-nuUo?utm_source=share&amp;utm_medium=member_desktop" target=&ldquo;blank&rdquo;>More Video</a></p>
</div></div>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px"><source src="./images/Gem.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>GEM Vehicle Parking Navigation with Pipeline Design</h2>
<h3>Yan Miao, Arjun Ray, John Pohovey, Asher Mai</h3>
<h4><a href="https://publish.illinois.edu/safe-autonomy/" target=&ldquo;blank&rdquo;>ECE 484: Principles of Safe Autonomy</a></h4>
<p>We designed a pipeline that enables autonomous parking lot navigation on the Gem Vehicle platform.
The pipeline includes perception with LiDAR and YOLOv5, path planning with Hybrid A-star, and control using PID.
We tested our system with different obstacles in the parking lot, and evaluated the parking success using 
Intersection Over Union (IOU) metric. We tested how well the PID control follows the planned trajectory using
Dynamic Time Warping (DTW).</p>
<p><a href="https://www.youtube.com/watch?list=PLcA4s4DKSOF12iLaJmmJ_2ZoCD6aFd9Ct&amp;v=-f7Qd7TnaDU" target=&ldquo;blank&rdquo;>Youtube</a> /
<a href="https://docs.google.com/presentation/d/1qyt7nnVl4wSo1H0AG19ofFyk_EndlIzEOxcE05svHTk/edit?usp=sharing" target=&ldquo;blank&rdquo;>Presentation</a></p>
</div></div>
<h2>2021</h2>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px"><source src="./images/crossy.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>Crossy Road Game on FPGA</h2>
<h3>Asher Mai, Amanda Favila</h3>
<h4>ECE 385: Digital Systems Laboratory</h4>
<p>We recreated the popular iOS game Crossy Road on FPGA using SystemVerilog HDL (Hardware Description Language).
We designed our own game graphics down to pixel level and output VGA signal to display a 60 Hz, 480 by 640 pixel game.
One of the challenges was to randomly generate and efficiently store the essential game information such as all the car and tree positions. 
After brainstorming, we came up with the idea of using linear-feedback shift registers to solve this challenge.</p>
<p><a href="https://www.linkedin.com/posts/ashermai_after-a-month-of-workamanda-favilaand-activity-6797002446129524736-JILB?utm_source=share&amp;utm_medium=member_desktop" target=&ldquo;blank&rdquo;>LinkedIn Post</a></p>
</div></div>
<div class="infoblock">
<div class="blockcontent">
<p>&ldquo;Thanks for writing to me and showing me this. It's nice to hear that you enjoy Crossy!
Your project looks awesome, especially on a big screen!
All the best for your course and future projects.&rdquo; &#8201;&mdash;&#8201; <a href="https://andrewsum.com" target=&ldquo;blank&rdquo;>Andrew Sum</a>, Creator of Crossy Road</p>
</div></div>
<h2>2020</h2>
<div id="text-img-container"><div id="img-container">
<video autoplay loop muted playsinline alt="" width="280px"><source src="./images/ball.mp4" type="video/mp4" /></video></div>
<div id="text-container"><h2>Floating Glass Sphere</h2>
<h3>Asher Mai</h3>
<h4>Personal Project</h4>
<p>I was experimenting with 3D animation, and I had this idea of combining
frosted glass and changing shape keys that animates the shape of both 
the inner and outer sphere.</p>
<p>Modeled and animated with <a href="https://www.blender.org" target=&ldquo;blank&rdquo;>Blender</a>.</p>
</div></div>
<div id="text-img-container"><div id="img-container">
<img src="./images/computer.JPG" alt="" width="280px" /></div>
<div id="text-container"><h2>The Future of Computing</h2>
<h3>Asher Mai</h3>
<h4>Personal Project</h4>
<p>I had an imagination for what computers might look like in the future. You can have
windows floating around you, and you can drag them around. You can have multiple screens but 
without any monitors. This imagination has been brought to reality with the recent release
of Apple Vision Pro.</p>
<p>Modeled with <a href="https://www.blender.org" target=&ldquo;blank&rdquo;>Blender</a>.</p>
</div></div>
<h2>2019</h2>
<div id="text-img-container"><div id="img-container">
<img src="./images/endbound.jpeg" alt="" width="280px" /></div>
<div id="text-container"><h2>EndBound: A Calculus-based Board Game</h2>
<h3>Asher Mai, Ben Snyder, Jake Mulé, Jeremy Cheng</h3>
<h4>AP Calculus BC</h4>
<p>We designed a board game similar to Chutes and Ladders, but instead of using
dice roll to determine how far you move, you use Function cards and EndBound cards
to calculate the integral that determines how far you move.</p>
<p>Steps to move =</p>
<p style="text-align:center">
\[
    \int\limits_{0}^{(EndBound\;Card)} (Function\;Card) \;dx
\]
</p><p><a href="./assets/endBound_rules.pdf" target=&ldquo;blank&rdquo;>Rule Book</a></p>
</div></div>
<div class="bg-image" style="display:block;"></div>
<style>
    .bg-image{
        background-attachment: fixed;
        background-size: 100%;
        width: 100%;
        height: 100%;
        display: none;
        position: fixed;
        top: 0;
        left: 0;
        background: #fff url("./images/deepmind3.jpg") center no-repeat;
        background-size: cover;
        background-position: center center;
        z-index: 0;
    }

</style>
<style>
    div#layout-content-container {
        background-color: #F5F5F7 !important ;
    }

    div#text-img-container {
        background-color: #FFFFFF;
        border-radius: 20px;
        padding:20px;
        box-shadow: 0px 0px 15px gray;
        box-sizing: border-box;
        border-radius: 20px;
        box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .6); 
        position: relative;
        z-index: 1;
        background: inherit;
        overflow: hidden;
    }
    div#text-img-container:before {
        content: "";
        position: absolute;
        /* background: inherit; */
        z-index: -1;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        box-shadow: inset 0 0 2000px rgba(255, 255, 255, .5);
        backdrop-filter: blur(30px) !important;
        -webkit-backdrop-filter: blur(20px) !important;
        margin: -20px;
    }

    div#layout-content :not(.bg-image) {
        z-index: 1;
        position: relative;
    }

    div#subtitle, div#footer-text, div#layout-content > h2, h1{
        background-color: #FFFFFF;
        border-radius: 20px;
        padding:10px;
        box-shadow: 0px 0px 15px gray;
        box-sizing: border-box;
        border-radius: 20px;
        box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .6); 
        position: relative;
        z-index: 1;
        background: inherit;
        overflow: hidden;
    }

    div#subtitle:before, div#footer-text:before, div#layout-content > h2:before, h1:before {
        content: "";
        position: absolute;
        background: inherit;
        z-index: -1;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        box-shadow: inset 0 0 2000px rgba(255, 255, 255, .5);
        backdrop-filter: blur(10px) !important;
        -webkit-backdrop-filter: blur(10px) !important;
        margin: -20px;
    }

    div#layout-content > h2, h1 {
        display:inline-block;
    }

</style>
<script>
    document.title = "Asher's Projects"
</script>
</div> <!-- <div id="layout-content"> -->
<div id="footer-container">
<div id="footer">
<div id="footer-text">
Last edited  on February 13<sup>th</sup> 2024  05:16PM (Time Zone: CST). </br>
Powered by <a href="https://github.com/szl2/jemdoc-new-design" target="blank">jemdoc + new design      </a>.
Background images from <a href="https://deepmind.google/discover/visualising-ai/" target="blank">Google Deepmind</a>.
Professional headshot from <a href="https://www.shennyvisuals.com" target="blank">Shenny Visuals</a>.
</div> <!-- <div id="footer-text"> -->
</div> <!-- <div id="footer"> -->
</div> <!-- <div id="footer-container"> -->
</div> <!-- <div id="layout-content-container"> -->
</div> <!--- <div id="layout"> --->
</div> <!--- <div id="main-container"> --->
<script>
function openNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("openButton").innerHTML = "×";
        if (document.getElementById("layout-menu").style.width == "160px") {
            closeNav();
            return;
        }
        document.getElementById("layout-menu").style.width = "160px";
        // document.getElementById("layout-content-container").style.marginLeft = "140.8px";
        // document.getElementById("layout-content-container").style.position = "fixed";
    }
}
function closeNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "0";
        document.getElementById("layout-content-container").style.position = "static";
        document.getElementById("layout-content-container").style.marginLeft = "0px";
        document.getElementById("openButton").innerHTML = "☰";
        // setInterval(
        //     function(){ location.reload() },
        //     500
        // );
    }
}
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Z5NB40Z8QS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-Z5NB40Z8QS');
</script>
</body>
</html>
